{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63b5cf0c-cf72-4899-85f8-ffaee9642653",
   "metadata": {},
   "source": [
    "# Intelligent Content Retrieval System\n",
    "## Web Scraping and Vector Database Assignment\n",
    "\n",
    "**Author:** Phillemon Senoamadi  \n",
    "**Date:** December 2025\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d45d17-873e-47a4-ad43-f03d932cc2a0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf331b8-0780-4877-a0e1-8209f87200b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "import chromadb\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88102dd-0e3b-4d41-bc89-7291ebc56e6e",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1be3b26-1858-4ca0-b816-714ffec91dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; ContentRetrievalBot/1.0; +educational)\"}\n",
    "REQUEST_DELAY = 3  # seconds (rate limiting)\n",
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76511957-177b-43a8-ab11-981a0c6d0e35",
   "metadata": {},
   "source": [
    "## Website Selection\n",
    "\n",
    "Four South African websites were selected to ensure:\n",
    "- Public accessibility\n",
    "- English-language content\n",
    "- Diverse categories\n",
    "- Sufficient textual volume\n",
    "\n",
    "Categories covered:\n",
    "- News\n",
    "- Technology\n",
    "- Education\n",
    "- Legal / Public Information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923a64a6-d01b-4254-b5e7-cdce68b7b490",
   "metadata": {},
   "source": [
    "### List of Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3a928f-d21f-436b-9b9e-0cd6992e9f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = [\n",
    "    {\n",
    "        \"name\": \"WHO\",\n",
    "        \"url\": \"https://data.who.int/dashboards/covid19/cases?n=c\",\n",
    "        \"category\": \"Government \",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Microsoft\",\n",
    "        \"url\": \"https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/ai-agent-design-patterns?toc=%2Fazure%2Fdeveloper%2Fai%2Ftoc.json&bc=%2Fazure%2Fdeveloper%2Fai%2Fbreadcrumb%2Ftoc.json\",\n",
    "        \"category\": \"Technology \",\n",
    "    },\n",
    "    {\n",
    "        \"name\":\"apnews\",\n",
    "        \"url\":\"https://apnews.com/\",\n",
    "        \"category\" :\"News\"\n",
    "    },\n",
    "    {\n",
    "        \"name\":\"towardsdatascience\",\n",
    "        \"url\":\"https://towardsdatascience.com/solving-a-constrained-project-scheduling-problem-with-quantum-annealing-d0640e657a3b/\",\n",
    "        \"category\" :\"Educational Resources\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d71c8b3-2926-4acd-b936-8ee248a42e34",
   "metadata": {},
   "source": [
    "### Fetch page Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a77de1cc-e80e-4235-93d0-54278844d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_page(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch HTML content from a URL using ethical scraping practices.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "    response.raise_for_status()\n",
    "    time.sleep(REQUEST_DELAY)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4394cc2f-d79b-44c5-a01e-c1719fde94d5",
   "metadata": {},
   "source": [
    "### Clean Text Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b4a3ce-12fd-4626-8b11-2d3df988790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract visible text from HTML and clean it.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"lxml\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"nav\"]):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text(separator=\" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2085e0f6-5ae9-4121-a2e7-fe68ed411390",
   "metadata": {},
   "source": [
    "## Scrape Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bd3d136-b75f-43b6-98e5-c3b564e522c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping websites:   0%|                                                                         | 0/4 [00:00<?, ?it/s]C:\\Users\\f8872908\\AppData\\Local\\Temp\\ipykernel_22920\\430799487.py:12: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n",
      "Scraping websites:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž                                                | 1/4 [00:03<00:09,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHO scraped: 18,708 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping websites:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 2/4 [00:06<00:06,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft scraped: 41,278 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping websites:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 3/4 [00:10<00:03,  3.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apnews scraped: 31,324 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scraping websites: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "towardsdatascience scraped: 38,014 characters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scraped_data = []\n",
    "\n",
    "for site in tqdm(websites, desc=\"Scraping websites\"):\n",
    "    try:\n",
    "        html = fetch_page(site[\"url\"])\n",
    "        text = extract_text(html)\n",
    "        \n",
    "        scraped_data.append({\n",
    "            \"domain\": site[\"name\"],\n",
    "            \"url\": site[\"url\"],\n",
    "            \"category\": site[\"category\"],\n",
    "            \"timestamp\": datetime.utcnow().isoformat(),\n",
    "            \"text\": text,\n",
    "            \"char_count\": len(text)})\n",
    "        print(f\"{site['name']} scraped: {len(text):,} characters\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {site['name']}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a943874-73a2-45e9-9f18-0497daa10d82",
   "metadata": {},
   "source": [
    "## Scrapping Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d241600-a5c8-4b16-813d-ee334f833bd4",
   "metadata": {},
   "source": [
    "### Scraping Summary\n",
    "\n",
    "The table below shows the character count for each website to verify that\n",
    "each source exceeds the minimum 5,000-character requirement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f2256-2d80-4cb3-9776-4aaedc85ed93",
   "metadata": {},
   "source": [
    "## Scrapping Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34b688f1-c778-4ea0-9ad0-90e08ead6aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>category</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHO</td>\n",
       "      <td>Government</td>\n",
       "      <td>18708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Technology</td>\n",
       "      <td>41278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apnews</td>\n",
       "      <td>News</td>\n",
       "      <td>31324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>towardsdatascience</td>\n",
       "      <td>Educational Resources</td>\n",
       "      <td>38014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               domain               category  char_count\n",
       "0                 WHO            Government        18708\n",
       "1           Microsoft            Technology        41278\n",
       "2              apnews                   News       31324\n",
       "3  towardsdatascience  Educational Resources       38014"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scraped = pd.DataFrame(scraped_data)\n",
    "df_scraped[[\"domain\", \"category\", \"char_count\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f6880501-c3e2-486c-b2bf-5819bae49c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scraped['percentages'] = df_scraped['char_count']/df_scraped['char_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0ba7e37d-f78d-4c89-b856-f1489d64b334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>char_count</th>\n",
       "      <th>percentages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHO</td>\n",
       "      <td>https://data.who.int/dashboards/covid19/cases?n=c</td>\n",
       "      <td>Government</td>\n",
       "      <td>2026-01-11T08:20:48.200842</td>\n",
       "      <td>COVID-19 cases | WHO COVID-19 dashboard Skip t...</td>\n",
       "      <td>18708</td>\n",
       "      <td>0.144660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Microsoft</td>\n",
       "      <td>https://learn.microsoft.com/en-us/azure/archit...</td>\n",
       "      <td>Technology</td>\n",
       "      <td>2026-01-11T08:20:51.755113</td>\n",
       "      <td>AI Agent Orchestration Patterns - Azure Archit...</td>\n",
       "      <td>41278</td>\n",
       "      <td>0.319183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apnews</td>\n",
       "      <td>https://apnews.com/</td>\n",
       "      <td>News</td>\n",
       "      <td>2026-01-11T08:20:55.245993</td>\n",
       "      <td>Associated Press News: Breaking News, Latest H...</td>\n",
       "      <td>31324</td>\n",
       "      <td>0.242213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>towardsdatascience</td>\n",
       "      <td>https://towardsdatascience.com/solving-a-const...</td>\n",
       "      <td>Educational Resources</td>\n",
       "      <td>2026-01-11T08:20:58.687296</td>\n",
       "      <td>Solving a Constrained Project Scheduling Probl...</td>\n",
       "      <td>38014</td>\n",
       "      <td>0.293944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               domain                                                url  \\\n",
       "0                 WHO  https://data.who.int/dashboards/covid19/cases?n=c   \n",
       "1           Microsoft  https://learn.microsoft.com/en-us/azure/archit...   \n",
       "2              apnews                                https://apnews.com/   \n",
       "3  towardsdatascience  https://towardsdatascience.com/solving-a-const...   \n",
       "\n",
       "                category                   timestamp  \\\n",
       "0            Government   2026-01-11T08:20:48.200842   \n",
       "1            Technology   2026-01-11T08:20:51.755113   \n",
       "2                   News  2026-01-11T08:20:55.245993   \n",
       "3  Educational Resources  2026-01-11T08:20:58.687296   \n",
       "\n",
       "                                                text  char_count  percentages  \n",
       "0  COVID-19 cases | WHO COVID-19 dashboard Skip t...       18708     0.144660  \n",
       "1  AI Agent Orchestration Patterns - Azure Archit...       41278     0.319183  \n",
       "2  Associated Press News: Breaking News, Latest H...       31324     0.242213  \n",
       "3  Solving a Constrained Project Scheduling Probl...       38014     0.293944  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scraped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d26c8-82b2-41f7-90d7-b4d2a511cbcf",
   "metadata": {},
   "source": [
    "## Text Chunking Function `"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69d31f5d-0f26-4dd2-b972-cda30ee98f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str,chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05a1301-7160-42a0-924b-dd3c8f7825a2",
   "metadata": {},
   "source": [
    "## Create Chunked Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62bead73-e680-458b-b6ef-5cdba26e2559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 201\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "\n",
    "for row in scraped_data:\n",
    "    chunks = chunk_text(row[\"text\"])\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"domain\": row[\"domain\"],\n",
    "            \"url\": row[\"url\"],\n",
    "            \"category\": row[\"category\"],\n",
    "            \"timestamp\": row[\"timestamp\"],\n",
    "            \"chunk_id\": i,\n",
    "            \"text\": chunk,\n",
    "            \"chunk_length\": len(chunk)\n",
    "        })\n",
    "\n",
    "df_chunks = pd.DataFrame(all_chunks)\n",
    "print(f\"Total chunks created: {len(df_chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7027a794-5e52-4b3c-8a53-7a7caa401aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text</th>\n",
       "      <th>chunk_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHO</td>\n",
       "      <td>https://data.who.int/dashboards/covid19/cases?n=c</td>\n",
       "      <td>Government</td>\n",
       "      <td>2026-01-11T08:20:48.200842</td>\n",
       "      <td>0</td>\n",
       "      <td>COVID-19 cases | WHO COVID-19 dashboard Skip t...</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHO</td>\n",
       "      <td>https://data.who.int/dashboards/covid19/cases?n=c</td>\n",
       "      <td>Government</td>\n",
       "      <td>2026-01-11T08:20:48.200842</td>\n",
       "      <td>1</td>\n",
       "      <td>and Saba Bosnia and Herzegovina Botswana Braz...</td>\n",
       "      <td>800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  domain                                                url     category  \\\n",
       "0    WHO  https://data.who.int/dashboards/covid19/cases?n=c  Government    \n",
       "1    WHO  https://data.who.int/dashboards/covid19/cases?n=c  Government    \n",
       "\n",
       "                    timestamp  chunk_id  \\\n",
       "0  2026-01-11T08:20:48.200842         0   \n",
       "1  2026-01-11T08:20:48.200842         1   \n",
       "\n",
       "                                                text  chunk_length  \n",
       "0  COVID-19 cases | WHO COVID-19 dashboard Skip t...           800  \n",
       "1   and Saba Bosnia and Herzegovina Botswana Braz...           800  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9200fb8c-0b59-4e60-aea0-2bca1bae0ac9",
   "metadata": {},
   "source": [
    "## Chunk Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c33b637e-7ae9-4d78-a7c7-eb2ee01a6a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    201.000000\n",
       "mean     790.288557\n",
       "std       70.104895\n",
       "min      124.000000\n",
       "25%      800.000000\n",
       "50%      800.000000\n",
       "75%      800.000000\n",
       "max      800.000000\n",
       "Name: chunk_length, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunks[\"chunk_length\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce71242-ef14-4b05-8fe8-a18900ec5cf1",
   "metadata": {},
   "source": [
    "## End of Scraping & Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c895777-087d-4a5f-a2b5-96795c78d6ce",
   "metadata": {},
   "source": [
    "## 2. Data Collection and Text Processing Complete\n",
    "\n",
    "At this stage:\n",
    "- All websites were scraped ethically\n",
    "- HTML noise was removed\n",
    "- Text was chunked into 800â€“1200 character segments\n",
    "- Metadata was preserved for each chunk\n",
    "\n",
    "The processed corpus is now ready for embedding generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a490bb-86b3-4c20-b378-4755935cedb9",
   "metadata": {},
   "source": [
    "# Embedding Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f149fb7-cc6d-4e63-98f5-d333283da8ac",
   "metadata": {},
   "source": [
    "## Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2a22bda-60b2-4d66-aac5-fa2ff07b87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbed6551-7e20-48e7-93da-cc1b00de5a86",
   "metadata": {},
   "source": [
    "## Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b428b331-e88e-488d-a51d-063863ea724e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94363c6527d848a688823be823156ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: (201, 384) \n",
      " Embeddings: [[ 0.00674995  0.02543921 -0.05318063 ... -0.03135065 -0.02791882\n",
      "   0.03389486]\n",
      " [ 0.08636957  0.01803675 -0.05463221 ... -0.02771902  0.00385751\n",
      "  -0.0168611 ]\n",
      " [ 0.0194258   0.0223157  -0.04385505 ... -0.0583318  -0.01312458\n",
      "  -0.03841379]\n",
      " ...\n",
      " [-0.17394926 -0.08896809  0.07456005 ...  0.06617937 -0.0763445\n",
      "  -0.03236045]\n",
      " [-0.09830879 -0.09961621  0.01288439 ...  0.04623834 -0.08831649\n",
      "  -0.01794759]\n",
      " [-0.03202517 -0.10690522 -0.08798777 ...  0.02482592 -0.0690345\n",
      "   0.04047528]]\n"
     ]
    }
   ],
   "source": [
    "texts = df_chunks[\"text\"].tolist()\n",
    "\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True\n",
    ")\n",
    "\n",
    "print(\"Embedding shape:\", embeddings.shape, \"\\n\" ,\"Embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc285b5b-6b78-4be4-911e-6afae7612fcd",
   "metadata": {},
   "source": [
    "# VECTOR DATABASE (ChromaDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd061b-de58-4ecc-b888-7c4ded2b9fcb",
   "metadata": {},
   "source": [
    "## Create Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6d9efcc-041f-47b1-839c-7696769a8fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "collection = client.create_collection(\n",
    "    name=\"content_retrieval\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2466ae7-c9fc-4b1a-b9be-8ee8e5096efe",
   "metadata": {},
   "source": [
    "## Store Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5df00eb-c2a1-4a32-953c-ebebf6a4f6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored documents: 201\n"
     ]
    }
   ],
   "source": [
    "collection.add(\n",
    "    documents=df_chunks[\"text\"].tolist(),\n",
    "    embeddings=embeddings.tolist(),\n",
    "    metadatas=df_chunks[[\"domain\", \"url\", \"category\"]].to_dict(\"records\"),\n",
    "    ids=[str(i) for i in range(len(df_chunks))]\n",
    ")\n",
    "\n",
    "print(\"Stored documents:\", collection.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5399809f-674d-4ae9-bb63-e459a5269892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb35d969-0fc1-4fc5-b597-705823be6b64",
   "metadata": {},
   "source": [
    "# Semantic Search Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad799d8-3e57-40fc-87b2-fb6f6cba25ba",
   "metadata": {},
   "source": [
    "### Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f0b1f73-4a64-487b-83ef-90f06345cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str, top_k: int = 5):\n",
    "    query_embedding = model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    for i in range(top_k):\n",
    "        print(f\"\\nResult {i+1}\")\n",
    "        print(\"Score:\", results[\"distances\"][0][i])\n",
    "        print(\"domain:\", results[\"metadatas\"][0][i][\"domain\"])\n",
    "        print(\"Text:\", results[\"documents\"][0][i][:300], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3ced1c77-6403-4b95-ba7c-987c07795a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def semantic_search(query: str, top_k: int = 5, output_file: str = \"system_results.json\"):\n",
    "    # Encode query\n",
    "    query_embedding = model.encode([query], normalize_embeddings=True)\n",
    "\n",
    "    # Query vector database\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=top_k\n",
    "    )\n",
    "\n",
    "    # Build this run's result\n",
    "    run_results = {\n",
    "        \"query\": query,\n",
    "        \"top_k\": top_k,\n",
    "        \"timestamp\": datetime.utcnow().isoformat(),\n",
    "        \"results\": []\n",
    "    }\n",
    "\n",
    "    for i in range(top_k):\n",
    "        score = results[\"distances\"][0][i]\n",
    "        domain = results[\"metadatas\"][0][i].get(\"domain\")\n",
    "        text = results[\"documents\"][0][i]\n",
    "\n",
    "        print(f\"\\nResult {i+1}\")\n",
    "        print(\"Score:\", score)\n",
    "        print(\"domain:\", domain)\n",
    "        print(\"Text:\", text[:300], \"...\")\n",
    "\n",
    "        run_results[\"results\"].append({\n",
    "            \"rank\": i + 1,\n",
    "            \"score\": score,\n",
    "            \"domain\": domain,\n",
    "            \"text\": text\n",
    "        })\n",
    "\n",
    "    # Load existing file safely\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                existing_data = json.load(f)\n",
    "            except json.JSONDecodeError:\n",
    "                existing_data = []\n",
    "    else:\n",
    "        existing_data = []\n",
    "\n",
    "    # ðŸ”‘ Normalize storage format\n",
    "    if isinstance(existing_data, dict):\n",
    "        all_results = [existing_data]   # migrate old single-run format\n",
    "    elif isinstance(existing_data, list):\n",
    "        all_results = existing_data\n",
    "    else:\n",
    "        all_results = []\n",
    "\n",
    "    # Append new run\n",
    "    all_results.append(run_results)\n",
    "\n",
    "    # Write back\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"\\n Results appended to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3234e1-b9e4-45cc-b5a9-3540be5d2410",
   "metadata": {},
   "source": [
    "## Testing Semantic Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8dcc0880-ca6b-4ef9-bab9-f09fe8cc98a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1\n",
      "Score: 0.6415318250656128\n",
      "domain: apnews\n",
      "Text: g with oil executives in the East Room of the White House, Friday, Jan. 9, 2026, in Washington. (AP Photo/Alex Brandon) Trump signs executive order meant to protect the money from Venezuelan oil President Donald Trump has issued a new executive order to protect Venezuelan oil revenue from judicial p ...\n",
      "\n",
      "Result 2\n",
      "Score: 0.6860718131065369\n",
      "domain: apnews\n",
      "Text: n EspaÃ±ol Deportes Donald Trump Most watched videos Standards Quizzes Press Releases My Account AP News Code of Conduct Sign in Search Query Submit Search Show Search Menu Submit Search Bob Weir dies Alex Bregman contract Iran protests T.K. Carter dies at 69 Anti-ICE protests Menu World SECTIONS Isr ...\n",
      "\n",
      "Result 3\n",
      "Score: 0.6862151622772217\n",
      "domain: apnews\n",
      "Text: 10, 2026. (UGC via AP) Iran warns US troops and Israel will be targets if America strikes over protests as death toll rises Protests challenging Iranâ€™s theocracy have reached the two-week mark, with demonstrators flooding the streets in Tehran and Mashhad. 5 MIN READ MORE COVERAGE Footage circulatin ...\n",
      "\n",
      "Result 4\n",
      "Score: 0.708364725112915\n",
      "domain: apnews\n",
      "Text:  invest in Venezuela after Maduro ouster Doctors say changes to US vaccine recommendations are confusing parents and could harm kids WORLD NEWS Greenlandâ€™s party leaders firmly reject Trumpâ€™s push for US control of the island How the US could take over Greenland and the potential challenges Myanmar  ...\n",
      "\n",
      "Result 5\n",
      "Score: 0.739057719707489\n",
      "domain: apnews\n",
      "Text: fire, flood and â€˜plagueâ€™ Newsletters World of Faith Comprehensive global coverage of how religion shapes our world. See All Newsletters EspaÃ±ol SECTIONS Deportes Donald Trump TOP STORIES Â¿Puede una cÃ³moda convertirse en un armario de cocina? El arte de reutilizar muebles antiguos Argentina reembolsa ...\n",
      "\n",
      " Results appended to system_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f8872908\\AppData\\Local\\Temp\\ipykernel_22920\\249846900.py:19: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "semantic_search(\"Trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "72a77c7e-2f4e-4852-83c6-e257cb2d36df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1\n",
      "Score: 0.5820498466491699\n",
      "domain: Microsoft\n",
      "Text: e to the same problem. This collaboration typically occurs in scenarios that feature the following multi-agent decision-making techniques: Brainstorming Ensemble reasoning Quorum and voting-based decisions Time-sensitive scenarios where parallel processing reduces latency. When to avoid concurrent o ...\n",
      "\n",
      "Result 2\n",
      "Score: 0.5907529592514038\n",
      "domain: Microsoft\n",
      "Text:  processing inefficient or impossible. Agents can't reliably coordinate changes to shared state or external systems while running simultaneously. There's no clear conflict resolution strategy to handle contradictory or conflicting results from each agent. Result aggregation logic is too complex or l ...\n",
      "\n",
      "Result 3\n",
      "Score: 0.6375733017921448\n",
      "domain: Microsoft\n",
      "Text: g, all agents work in parallel, which reduces overall run time and provides comprehensive coverage of the problem space. This orchestration pattern resembles the Fan-out/Fan-in cloud design pattern. The results from each agent are often aggregated to return a final result, but that's not required. E ...\n",
      "\n",
      "Result 4\n",
      "Score: 0.6457217931747437\n",
      "domain: Microsoft\n",
      "Text: ext stage depends on Workflow stages that can't be parallelized Progressive refinement requirements, such as draft, review, polish workflows Systems where you understand the availability and performance characteristics of every AI agent in the pipeline, and where failures or delays in one AI agent's ...\n",
      "\n",
      "Result 5\n",
      "Score: 0.6656575202941895\n",
      "domain: Microsoft\n",
      "Text:  The available agents must know which agents are available for processing. This pattern supports both deterministic calls to all registered agents and dynamic selection of which agents to invoke based on the task requirements. When to use concurrent orchestration Consider the concurrent orchestratio ...\n",
      "\n",
      " Results appended to system_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f8872908\\AppData\\Local\\Temp\\ipykernel_22920\\249846900.py:19: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "semantic_search(\"parallelized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fa4d831b-a682-4365-9fd1-ec8e1209d38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1\n",
      "Score: 0.8164938688278198\n",
      "domain: WHO\n",
      "Text: reenland Grenada Guadeloupe Guam Guatemala Guernsey Guinea Guinea-Bissau Guyana Haiti Holy See Honduras Hungary Iceland India Indonesia Iran (Islamic Republic of) Iraq Ireland Isle of Man Israel Italy Jamaica Japan Jersey Jordan Kazakhstan Kenya Kiribati Kosovo (In accordance with UN Security Counci ...\n",
      "\n",
      "Result 2\n",
      "Score: 0.8229862451553345\n",
      "domain: WHO\n",
      "Text:  between the Governments of Argentina and the United Kingdom of Great Britain and Northern Ireland concerning sovereignty over the Falkland Islands (Malvinas). The mention of specific companies or of certain manufacturersâ€™ products does not imply that they are endorsed or recommended by WHO in prefe ...\n",
      "\n",
      "Result 3\n",
      "Score: 0.8350211381912231\n",
      "domain: WHO\n",
      "Text: f accessing or utilizing the Datasets with or without prior notice to you. Maps The designations employed and the presentation of the material in this publication do not imply the expression of any opinion whatsoever on the part of WHO concerning the legal status of any country, territory, city or a ...\n",
      "\n",
      "Result 4\n",
      "Score: 0.8368934392929077\n",
      "domain: WHO\n",
      "Text: Arabia Senegal Serbia Seychelles Sierra Leone Singapore Sint Maarten (Dutch part) Slovakia Slovenia Solomon Islands Somalia South Africa South Sudan Spain Sri Lanka Sudan Suriname Sweden Switzerland Syrian Arab Republic Tajikistan Thailand Timor-Leste Togo Tokelau Tonga Trinidad and Tobago Tunisia T ...\n",
      "\n",
      "Result 5\n",
      "Score: 0.8427192568778992\n",
      "domain: WHO\n",
      "Text: ccounts. Since 22 March 2020, global data has been compiled through WHO region-specific dashboards, and/or aggregate count data reported directly to WHO headquarters by Member States. Statistical counts include both domestic and repatriated cases. Case detection, definitions, testing strategies, rep ...\n",
      "\n",
      " Results appended to system_results.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\f8872908\\AppData\\Local\\Temp\\ipykernel_22920\\249846900.py:19: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"timestamp\": datetime.utcnow().isoformat(),\n"
     ]
    }
   ],
   "source": [
    "semantic_search(\"British\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fda7e2-a6cd-4edf-961a-8c36a8dae555",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
